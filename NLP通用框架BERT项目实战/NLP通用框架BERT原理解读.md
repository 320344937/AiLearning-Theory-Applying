###  NLP通用框架BERT原理解读

#### 传统解决方案遇到的问题

传统的RNN网络

![1609724393950](assets/1609724393950.png)

- 训练速度：无法加速训练，并行等
- Self-Attention机制（注意力），一段话中，不是每个词都重要，我们只需要关注重要的部分。如：下班后我们一起去吃饭吧，我听说有家面馆挺好吃的，我请客。是不是对于听的人来说主要是“我请客”。

- word2vec：训练好词向量就永久不变了，不同的语境相同的词相同的向量，但这合理吗？就想我们在生气的时候说傻子，很开心的时候说傻子，意思是完全不一样的，



#### Transformer整体架构如下

![1609725400828](assets/1609725400828.png)

#### 注意力机制的作用

- 对于输入的数据，我们的关注点是什么？
- 如何才能让计算机关注到这些有价值的信息？

![1609725559431](assets/1609725559431.png)

> 如上，传入一段文本，如果我们没有强调注意什么，那么词向量结果可能是平行的，如果我们强调“eating”，那么“eating”这个词的词向量就会有所不同。

如果是人为的加权，告诉计算机哪个重要，这显然是不合实际的，应该让计算机自己发现哪些重要。

![1609725763643](assets/1609725763643.png)

> “it”在第一句中是指代“animal”，表示它太累了没有过去。
>
> “it”在第二句中指代“street”，表示路太窄了没有过去。
>
> 这里关注的是“animal”，我们希望即使是第二句，“animal”对结果的影响越大。



#### Self-Attention计算

- 输入经过编码后得到的向量。
- 得到当前词语上下文的关系，可以当做是加权。
- 构建三个矩阵分别查询当前词跟其它词的关系，以及特征向量的表达。

如下图：

![1609726549882](assets/1609726549882.png)

> 先转换成向量，构建三个矩阵Q、K、V，求出来第一个词编码的时候怎么找到上下文。右边的W就是权重。

这三个矩阵具体做什么：

- Q: query，要去查询的
- K: key，等着被查的
- V: value，实际的特征信息

![1609726848038](assets/1609726848038.png)

> X是输入内容，结果W后，形成Q、K、V不同矩阵的特征向量。

![1609730605624](assets/1609730605624.png)

> q与k的内积表示有多匹配，如果Xa与Xb之间无关的时候，那么其在坐标系上的表示是垂直的；如果有关系，则非垂直，则有夹角有内积，相关性越大，则夹角越小，内积越大。

