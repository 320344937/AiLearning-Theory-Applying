###  NLP通用框架BERT原理解读

#### 传统解决方案遇到的问题

传统的RNN网络

![1609724393950](assets/1609724393950.png)

- 训练速度：无法加速训练，并行等
- Self-Attention机制（注意力），一段话中，不是每个词都重要，我们只需要关注重要的部分。如：下班后我们一起去吃饭吧，我听说有家面馆挺好吃的，我请客。是不是对于听的人来说主要是“我请客”。

- word2vec：训练好词向量就永久不变了，不同的语境相同的词相同的向量，但这合理吗？就想我们在生气的时候说傻子，很开心的时候说傻子，意思是完全不一样的，