# 5.决策树——每次选一边

Decision tree



### 知识树

Knowledge tree

![1618575672562](assets/1618575672562.png)



### 一个小故事

A story

挑苹果：

![1618575710142](assets/1618575710142.png)

> 根据这些特征，如颜色是否是红色、硬度是否是硬、香味是否是香，如果全部满足绝对是好苹果，或者红色+硬但是无味也是好苹果，从上图可以看出来，只要做足够的循环判断即可得到结果。

如下图：

![1618576033128](assets/1618576033128.png)

> 一步步走下来，就能挑到好苹果。这就是决策树

1. 最顶端的叫根节点，所有样本的预测都是从根节点开始。
2. 每一个圆形节点表示判断，每个节点只对样本的某个属性进行判断。
3. 圆形节点是标记节点，走到圆形节点表示判断结束，将圆形节点中的标签作为对应的预测结果。

如何构建决策树：

1. 构建的决策树按顺序对每个特征进行判断（低效）
2. 每个判断节点都尽可能让一半进入A分支，另一半进入B分支（高效）

引入新的知识，信息熵



### 信息熵

Information entropy

1. 每走一步，我们都在确定苹果的好坏。
2. 在根节点时，我们对苹果的好坏一无所知。
3. 经过对颜色的判断后，如果是红色，我们明白好坏的概率是1/2。虽然还包含了1/2的不确定性。
4. 如果苹果红色的前提下又硬，我们100%确定它是好苹果。此时不确定性坍塌为0。
5. 这是一个减少不确定性的过程。

从整体来讲，我们希望决策树每走一步，不确定性都下降的快一些，让我们的判断步数无限小。

**什么是信息的不确定性？**

就是信息熵

在信息论与概率统计中，熵（entropy）是表示随机变量不确定性的度量，设X是一个取有限个值的离散随机变量，其概率分布为

![1618576749908](assets/1618576749908.png)

则随机变量X的熵定义为

![1618576770871](assets/1618576770871.png)

> 面试可能会问到这个公式，还有交叉熵、相对熵

熵越大，则随机变量的不确定性越大。其中0 ≤ H(P) ≤ log n



### 举例计算

Example

假设投色子，6个的概率分别是1/6，计算如下：

![1618577639913](assets/1618577639913.png)

> 其中6个1/6（log左边的六分之一）加起来就是1

![1618577661694](assets/1618577661694.png)

> ![1618577700910](assets/1618577700910.png)

则最终=log6

这也解释了为什么上面H(P) ≤ log n

另外，均由分布的时候，熵最大，因为所有可能都是一样的，如上面的6个面都是1/6。



如果有1个坏苹果和9个好苹果时，我们可以认为大部分都是坏苹果。内部并不混乱，确定性很大，熵很小。

